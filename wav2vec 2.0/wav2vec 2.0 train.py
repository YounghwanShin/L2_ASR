
import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.nn import CTCLoss
from wav2vec import NaiveWav2Vec2PhonemeModel
from data import PhonemeRecognitionDataset
import os
import sys
import json
import logging
import argparse
import random
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import CTCLoss
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader

from wav2vec import NaiveWav2Vec2PhonemeModel
from data import PhonemeRecognitionDataset
from evaluate import evaluate_phoneme_recognition

'''
# ì•„ë˜ ì£¼ì„ì²˜ë¦¬ê°€ ì›ë˜ ì‘ì„± ì½”ë“œ -> í•™ìŠµì´ ë˜ê¸´ í•˜ë‚˜ ë„ˆë¬´ ê°€ì¤‘ì¹˜ ë‚®ì•„ì„œ ë¬¸ì œ ì´ê±´ ì˜í™˜ë‹˜ê±° ì¤‘ í•„ìš”í•œê±° ê°€ì ¸ì˜¨ê±°

def seed_everything(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def phoneme_collate_fn(batch):
    waveforms, phoneme_labels, label_lengths, wav_files = zip(*batch)

    max_audio_len = max([waveform.shape[0] for waveform in waveforms])
    padded_waveforms = [
        torch.nn.functional.pad(w, (0, max_audio_len - w.shape[0])) for w in waveforms
    ]
    padded_waveforms = torch.stack(padded_waveforms)

    max_label_len = max([lbl.shape[0] for lbl in phoneme_labels])
    padded_labels = [
        torch.nn.functional.pad(l, (0, max_label_len - l.shape[0]), value=0) for l in phoneme_labels
    ]
    padded_labels = torch.stack(padded_labels)

    audio_lengths = torch.tensor([w.shape[0] for w in waveforms])
    label_lengths = torch.tensor(label_lengths)

    return padded_waveforms, padded_labels, audio_lengths, label_lengths, wav_files


def train(model, dataloader, criterion, optimizer, device, epoch, max_grad_norm=1.0):
    model.train()
    running_loss = 0.0
    pbar = tqdm(dataloader, desc=f'Epoch {epoch} [Train]')

    for batch_idx, (waveforms, phoneme_labels, audio_lengths, label_lengths, _) in enumerate(pbar):
        waveforms, phoneme_labels = waveforms.to(device), phoneme_labels.to(device)
        audio_lengths, label_lengths = audio_lengths.to(device), label_lengths.to(device)

        attention_mask = (torch.arange(waveforms.shape[1]).unsqueeze(0).to(device) < audio_lengths.unsqueeze(1)).float()
        phoneme_logits = model(waveforms, attention_mask)

        log_probs = torch.log_softmax(phoneme_logits, dim=-1)
        input_seq_len, output_seq_len = waveforms.size(1), phoneme_logits.size(1)
        input_lengths = torch.floor((audio_lengths.float() / input_seq_len) * output_seq_len).long()
        input_lengths = torch.clamp(input_lengths, min=1, max=output_seq_len)

        loss = criterion(log_probs.transpose(0, 1), phoneme_labels, input_lengths, label_lengths)
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()

        running_loss += loss.item()
        pbar.set_postfix(loss=running_loss / (batch_idx + 1))

    return running_loss / len(dataloader)


def validate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    with torch.no_grad():
        pbar = tqdm(dataloader, desc='Validation')
        for batch_idx, (waveforms, phoneme_labels, audio_lengths, label_lengths, _) in enumerate(pbar):
            waveforms, phoneme_labels = waveforms.to(device), phoneme_labels.to(device)
            audio_lengths, label_lengths = audio_lengths.to(device), label_lengths.to(device)

            attention_mask = (torch.arange(waveforms.shape[1]).unsqueeze(0).to(device) < audio_lengths.unsqueeze(1)).float()
            phoneme_logits = model(waveforms, attention_mask)

            log_probs = torch.log_softmax(phoneme_logits, dim=-1)
            input_seq_len, output_seq_len = waveforms.size(1), phoneme_logits.size(1)
            input_lengths = torch.floor((audio_lengths.float() / input_seq_len) * output_seq_len).long()
            input_lengths = torch.clamp(input_lengths, min=1, max=output_seq_len)

            loss = criterion(log_probs.transpose(0, 1), phoneme_labels, input_lengths, label_lengths)
            running_loss += loss.item()
            pbar.set_postfix(val_loss=running_loss / (batch_idx + 1))

    return running_loss / len(dataloader)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--phoneme_train_data', type=str, required=True)
    parser.add_argument('--phoneme_val_data', type=str, required=True)
    parser.add_argument('--phoneme_map', type=str, required=True)

    parser.add_argument('--output_dir', type=str, default='models')
    parser.add_argument('--result_dir', type=str, default='results')
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--learning_rate', type=float, default=5e-5)
    parser.add_argument('--num_epochs', type=int, default=10)
    parser.add_argument('--max_audio_length', type=int, default=None)
    parser.add_argument('--use_scheduler', action='store_true')

    args = parser.parse_args()

    seed_everything(args.seed)
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(args.result_dir, exist_ok=True)

    logging.basicConfig(
        format='%(asctime)s - %(levelname)s - %(message)s',
        level=logging.INFO,
        handlers=[
            logging.FileHandler(os.path.join(args.result_dir, 'train.log')),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)

    with open(args.phoneme_map, 'r') as f:
        phoneme_to_id = json.load(f)

    num_phonemes = len(phoneme_to_id)
    model = NaiveWav2Vec2PhonemeModel(num_phonemes=num_phonemes).to(args.device)

    train_dataset = PhonemeRecognitionDataset(args.phoneme_train_data, phoneme_to_id, max_length=args.max_audio_length)
    val_dataset = PhonemeRecognitionDataset(args.phoneme_val_data, phoneme_to_id, max_length=args.max_audio_length)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=phoneme_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=phoneme_collate_fn)

    criterion = CTCLoss(blank=0, reduction='mean')
    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)

    scheduler = None
    if args.use_scheduler:
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6)

    best_val_loss = float('inf')

    for epoch in range(1, args.num_epochs + 1):
        logger.info(f"Epoch {epoch} / {args.num_epochs}")
        train_loss = train(model, train_loader, criterion, optimizer, args.device, epoch)
        val_loss = validate(model, val_loader, criterion, args.device)

        if scheduler:
            scheduler.step(val_loss)

        logger.info(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        torch.save(model.state_dict(), os.path.join(args.output_dir, f'last_model.pth'))

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), os.path.join(args.output_dir, f'best_model.pth'))
            logger.info("Saved best model.")

    logger.info("Training complete.")


if __name__ == "__main__":
    main()


'''

# ì²«ë²ˆì§¸ ë³´ë˜ ì½”ë“œë“œ
# 2ì°¨ ì¼ì–´ë‚œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ - ê¸¸ì´ ë¬¸ì œëŠ” í•´ê²° -> ë‹¤ë§Œ ë°°ì¹˜ ì¤„ì—¬ë„ í•™ìŠµì‹œ ê³„ì† out of memory error ë°œìƒ

# ê²½ë¡œ ì„¤ì •
train_json_path = "/home/ellt/Workspace/wav2vec/wav2vec 2.0/split_data/train.json"
val_json_path = "/home/ellt/Workspace/wav2vec/wav2vec 2.0/split_data/val.json"
phoneme_map_path = "/home/ellt/Workspace/wav2vec/wav2vec 2.0/split_data/phoneme_to_id.json"
output_dir = "/home/ellt/Workspace/wav2vec/wav2vec 2.0/checkpoints"


# í•˜ì´í¼íŒŒë¼ë¯¸í„°
epochs = 10
batch_size = 4 #ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •í•´ë„ í•´ê²°ì´ oom í•´ê²°ì´ ì•ˆë¨
learning_rate = 5e-5
device = "cuda" if torch.cuda.is_available() else "cpu"


# ì„œë¡œ ë‹¤ë¥¸ ë¼ë²¨ ë° ì˜¤ë””ì˜¤ ë¬¶ëŠ” ë°©ë²•
def collate_fn(batch):
    waveforms, labels, _, _ = zip(*batch)

    # ìë¥´ì§€ ì•ŠìŒ â†’ ì›ë³¸ ì „ì²´ waveform ì‚¬ìš©
    #MAX_AUDIO_LENGTH = 160000
    #waveforms = [x[:MAX_AUDIO_LENGTH] for x in waveforms]

    label_lengths = [x.shape[0] for x in labels]
    input_lengths = [x.shape[0] for x in waveforms]

    max_audio_len = max(input_lengths)
    padded_waveforms = torch.stack([
        torch.nn.functional.pad(x, (0, max_audio_len - x.shape[0])) for x in waveforms
    ])

    max_label_len = max(label_lengths)
    padded_labels = torch.stack([
        torch.nn.functional.pad(x, (0, max_label_len - x.shape[0]), value=0) for x in labels
    ])

    return (
        padded_waveforms,
        padded_labels,
        torch.tensor(input_lengths, dtype=torch.long),
        torch.tensor(label_lengths, dtype=torch.long)
    )


# í•™ìŠµ í•¨ìˆ˜
def train():
    batch_size = 4
    with open(phoneme_map_path, 'r') as f:
        phoneme_to_id = json.load(f)
    num_phonemes = len(phoneme_to_id)

    print(f"Using device: {device}")
    
    #ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model = NaiveWav2Vec2PhonemeModel(num_phonemes=num_phonemes).to(device)

    train_dataset = PhonemeRecognitionDataset(train_json_path, phoneme_to_id)
    val_dataset = PhonemeRecognitionDataset(val_json_path, phoneme_to_id)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

    criterion = CTCLoss(blank=0, zero_infinity=True)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

    os.makedirs(output_dir, exist_ok=True)
    best_val_loss = float("inf")

    for epoch in range(1, epochs + 1):
        print(f"\nğŸŸ¢ Epoch {epoch} ì‹œì‘ ------------------------")
        torch.cuda.empty_cache()
        model.train()
        total_train_loss = 0.0
        for waveforms, labels, input_lengths, label_lengths in train_loader:
            waveforms = waveforms.to(device)
            labels = labels.to(device)
            input_lengths = input_lengths.to(device)
            label_lengths = label_lengths.to(device)

            #labels = torch.cat([label for label in labels]).to(device)  # 1Dë¡œ ì´ì–´ë¶™ì´ê¸°-> (ì¢€ ì „ì— ìˆ˜ì •ì •)
            optimizer.zero_grad()
            logits = model(waveforms)                           # (B, T, C)
            log_probs = torch.log_softmax(logits, dim=-1)
            #log_probs = torch.log_softmax(logits, dim=-1).transpose(0, 1)
            batch_size = log_probs.size(1)                      # âœ… ì‹¤ì œ ë°°ì¹˜ í¬ê¸°
            seq_len = log_probs.size(0)                         # âœ… ì‹œí€€ìŠ¤ ê¸¸ì´

            output_lengths = torch.full(
                size=(batch_size,),                             # âœ… ë°˜ë“œì‹œ (B,) í¬ê¸°
                fill_value=seq_len,
                dtype=torch.long
            ).to(device)

          
          #  print("log_probs shape:", log_probs.transpose(0, 1).shape)
          #  print("labels shape:", labels.shape)
          #  print("output_lengths shape:", output_lengths.shape)
          #  print("label_lengths shape:", label_lengths.shape)
          #  print("label_lengths:", label_lengths)
            
            # (T, B, C) â† CTCLossê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹
            loss = criterion(log_probs, labels, output_lengths, label_lengths)
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)
        print(f"Epoch {epoch} | Train Loss: {avg_train_loss:.4f}")
        torch.cuda.empty_cache()
      
        # ê²€ì¦ ë‹¨ê³„
        model.eval()
        total_val_loss = 0.0

        with torch.no_grad():
            for waveforms, labels, input_lengths, label_lengths in val_loader:
              waveforms = waveforms.to(device)
              labels = labels.to(device)
              input_lengths = input_lengths.to(device)
              label_lengths = label_lengths.to(device)
              #labels = torch.cat([label for label in labels]).to(device)
              logits = model(waveforms)  # logits: (B, T, C)
              log_probs = torch.log_softmax(logits, dim=-1)

              current_batch_size = log_probs.size(1)
              seq_len = log_probs.size(0)

              output_lengths = torch.full(
                  size=(log_probs.size(1),),  #í˜„ì¬ ë°°ì¹˜ í¬ê¸°
                  fill_value=log_probs.size(0),  #ì‹œí€€ìŠ¤ ê¸¸ì´
                  dtype=torch.long
                ).to(device)


              loss = criterion(log_probs, labels, output_lengths, label_lengths)
              total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        print(f"Epoch {epoch} | Val Loss: {avg_val_loss:.4f}")

        torch.cuda.empty_cache()
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_path = os.path.join(output_dir, "best_model.pth")
            torch.save(model.state_dict(), best_model_path)
            print(f"Best model saved (epoch {epoch}, val loss: {best_val_loss:.4f})")

if __name__ == "__main__":
    train()
