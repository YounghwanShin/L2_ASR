import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.nn import CTCLoss
from wav2vec import NaiveWav2Vec2PhonemeModel
from data import PhonemeRecognitionDataset

# ì›ë˜ ë°°ì¹˜ 8ë¡œ í•˜ë©´ out of memory ë¬¸ì œ ë°œìƒ -> ë°°ì¹˜ ìë¥´ë©´ ì—í­1 ëë‚˜ê³  ì¸í’‹ ê¸¸ì´ë‘ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
# ë‹¬ë¼ì„œ ì—í­ 2ë¡œ ëª»ë„˜ì–´ê°. MAx audio length ì§€ì •í•´ì£¼ë©´ ë§ˆì°¬ê°€ì§€ë¡œ ë§‰íŒì— ê¸¸ì´ ë¶ˆì¼ì¹˜ ë¬¸ì œ.
# í•´ê²°ì´ ì•ˆëœ ìƒíƒœ.

# ê²½ë¡œ ì„¤ì •
train_json_path = "/home/ellt/Workspace/wav2vec/wav2vec 2.0/split_data/train.json"
val_json_path = "/home/ellt/Workspace/wav2vec/wav2vec 2.0/split_data/val.json"
phoneme_map_path = "/home/ellt/Workspace/wav2vec/wav2vec 2.0/split_data/phoneme_to_id.json"
output_dir = "/home/ellt/Workspace/wav2vec/wav2vec 2.0/checkpoints"


# í•˜ì´í¼íŒŒë¼ë¯¸í„°
epochs = 10
batch_size = 4 #ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •í•´ë„ í•´ê²°ì´ oom í•´ê²°ì´ ì•ˆë¨
learning_rate = 5e-5
device = "cuda" if torch.cuda.is_available() else "cpu"


# ì„œë¡œ ë‹¤ë¥¸ ë¼ë²¨ ë° ì˜¤ë””ì˜¤ ë¬¶ëŠ” ë°©ë²•
def collate_fn(batch):
    waveforms, labels, _, _ = zip(*batch)

    # ìë¥´ì§€ ì•ŠìŒ â†’ ì›ë³¸ ì „ì²´ waveform ì‚¬ìš©
    #MAX_AUDIO_LENGTH = 160000
    #waveforms = [x[:MAX_AUDIO_LENGTH] for x in waveforms]

    label_lengths = [x.shape[0] for x in labels]
    input_lengths = [x.shape[0] for x in waveforms]

    max_audio_len = max(input_lengths)
    padded_waveforms = torch.stack([
        torch.nn.functional.pad(x, (0, max_audio_len - x.shape[0])) for x in waveforms
    ])

    max_label_len = max(label_lengths)
    padded_labels = torch.stack([
        torch.nn.functional.pad(x, (0, max_label_len - x.shape[0]), value=0) for x in labels
    ])

    return (
        padded_waveforms,
        padded_labels,
        torch.tensor(input_lengths, dtype=torch.long),
        torch.tensor(label_lengths, dtype=torch.long)
    )


# í•™ìŠµ í•¨ìˆ˜
def train():
    batch_size = 4
    with open(phoneme_map_path, 'r') as f:
        phoneme_to_id = json.load(f)
    num_phonemes = len(phoneme_to_id)

    print(f"Using device: {device}")
    
    #ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model = NaiveWav2Vec2PhonemeModel(num_phonemes=num_phonemes).to(device)

    train_dataset = PhonemeRecognitionDataset(train_json_path, phoneme_to_id)
    val_dataset = PhonemeRecognitionDataset(val_json_path, phoneme_to_id)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

    criterion = CTCLoss(blank=0, zero_infinity=True)
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

    os.makedirs(output_dir, exist_ok=True)
    best_val_loss = float("inf")

    for epoch in range(1, epochs + 1):
        print(f"\nğŸŸ¢ Epoch {epoch} ì‹œì‘ ------------------------")
        torch.cuda.empty_cache()
        model.train()
        total_train_loss = 0.0
        for waveforms, labels, input_lengths, label_lengths in train_loader:
            waveforms = waveforms.to(device)
            labels = labels.to(device)
            input_lengths = input_lengths.to(device)
            label_lengths = label_lengths.to(device)

            optimizer.zero_grad()
            logits = model(waveforms)                           # (B, T, C)
            log_probs = torch.log_softmax(logits, dim=-1)

            batch_size = log_probs.size(1)                      # âœ… ì‹¤ì œ ë°°ì¹˜ í¬ê¸°
            seq_len = log_probs.size(0)                         # âœ… ì‹œí€€ìŠ¤ ê¸¸ì´

            output_lengths = torch.full(
                size=(batch_size,),                             # âœ… ë°˜ë“œì‹œ (B,) í¬ê¸°
                fill_value=seq_len,
                dtype=torch.long
            ).to(device)

          
            print("log_probs shape:", log_probs.transpose(0, 1).shape)
            print("labels shape:", labels.shape)
            print("output_lengths shape:", output_lengths.shape)
            print("label_lengths shape:", label_lengths.shape)
            print("label_lengths:", label_lengths)
            
            # (T, B, C) â† CTCLossê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹
            loss = criterion(log_probs, labels, output_lengths, label_lengths)
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)
        print(f"Epoch {epoch} | Train Loss: {avg_train_loss:.4f}")
        torch.cuda.empty_cache()
      
        # ê²€ì¦ ë‹¨ê³„
        model.eval()
        total_val_loss = 0.0

        with torch.no_grad():
            for waveforms, labels, input_lengths, label_lengths in val_loader:
              waveforms = waveforms.to(device)
              labels = labels.to(device)
              input_lengths = input_lengths.to(device)
              label_lengths = label_lengths.to(device)

              logits = model(waveforms)  # logits: (B, T, C)
              log_probs = torch.log_softmax(logits, dim=-1)

              current_batch_size = log_probs.size(1)
              seq_len = log_probs.size(0)

              output_lengths = torch.full(
                  size=(log_probs.size(0),),  #í˜„ì¬ ë°°ì¹˜ í¬ê¸°
                  fill_value=log_probs.size(1),  #ì‹œí€€ìŠ¤ ê¸¸ì´
                  dtype=torch.long
                ).to(device)


              loss = criterion(log_probs, labels, output_lengths, label_lengths)
              total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        print(f"Epoch {epoch} | Val Loss: {avg_val_loss:.4f}")

        torch.cuda.empty_cache()
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_path = os.path.join(output_dir, "best_model.pth")
            torch.save(model.state_dict(), best_model_path)
            print(f"Best model saved (epoch {epoch}, val loss: {best_val_loss:.4f})")

if __name__ == "__main__":
    train()
